accelerate launch --multi_gpu --num_machines 1 --num_processes 8 rm_training.py \
 --model_name {YOUR_MODEL_PATH} \
 --tokenizer_name {YOUR_TOKENIZER_PATH} \
 --resume_from_checkpoint False \
 --dataset_name {YOUR_DATASET_PATH} \
 --load_from_json False \
 --train_subset "data/reward" \
 --eval_subset "data/evaluation" \
 --train_subset 100000 \
 --eval_subset 1500 \
 --output_dir {REWARD_MODEL_SAVE_PATH} \
 --eval_first_step False \
 --per_device_train_batch_size 5 \
 --per_device_eval_batch_size 1 \
 --gradient_accumulation_steps 1 \
 --learning_rate 2e-5 \
 --weight_decay 0.0 \
 --bf16 True \
 --num_train_epochs 2 \
 --gradient_checkpointing False \
 --optim "adamw_hf" \
 --lr_scheduler_type "linear" \
 --max_length "512" \
 --evaluation_strategy "steps" \
 --eval_steps 2500 \
 --save_strategy "steps" \
 --save_steps 2500 \
 --remove_unused_columns False \
 --label_names "[]" \
 --logging_strategy "steps" \
 --logging_steps 10 \
 --report_to "wandb" \