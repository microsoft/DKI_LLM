[
    {
        "thought": "By encouraging the LLM to think step by step rather than directly outputting a code, chain-of-thought reasoning enables complex problem-solving through intermediate steps. This practice improves the model's ability to handle tasks that require deeper reasoning and provides insight into its decision-making process.",
        "name": "Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for the Chain-of-Thought (CoT) approach\n    # It is an important practice that allows the LLM to think step by step before solving the task.\n    cot_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instantiate a new LLM agent specifically for CoT\n    # To allow LLM thinking before answering, we need to set an additional output field 'thinking'.\n    cot_agent = LLMAgentBase(['thinking', 'code'], 'Chain-of-Thought Agent')\n\n    # Prepare the inputs for the CoT agent\n    # The input should be a list of Info, and the first one is often the taskInfo\n    cot_agent_inputs = [taskInfo]\n\n    # Get the response from the CoT agent\n    thinking, code = cot_agent(cot_agent_inputs, cot_instruction)\n\n    # Return only the final code\n    return code\n",
        "generation": "initial",
        "test_fitness": "90.8%"
    },
    {
        "thought": "To enhance its performance, an LLM can iteratively improve its code based on feedback. By reflecting on its previous attempts and incorporating feedback, the model can refine its reasoning and provide a more accurate answer.",
        "name": "Self-Refine (Reflexion)",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for reflecting on previous attempts and feedback to improve\n    cot_reflect_instruction = \"Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better.\"\n    cot_agent = LLMAgentBase(['thinking', 'code'], 'Chain-of-Thought Agent')\n\n    # Instruction for providing feedback and correcting the code\n    critic_instruction = \"Please review the code above and criticize on where might be wrong. If you are absolutely sure it is correct, output 'True' in 'correct'.\"\n    critic_agent = LLMAgentBase(['feedback', 'correct'], 'Critic Agent')\n    \n    N_max = 5 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    thinking, code = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    for i in range(N_max):\n        # Get feedback and correct status from the critic\n        feedback, correct = critic_agent([taskInfo, thinking, code], critic_instruction, i)\n        if correct.content == 'True':\n            break\n            \n        # Add feedback to the inputs for the next iteration\n        cot_inputs.extend([thinking, code, feedback])\n\n        # Reflect on previous attempts and refine the code\n        thinking, code = cot_agent(cot_inputs, cot_reflect_instruction, i + 1)\n    return code\n",
        "generation": "initial",
        "test_fitness": "86.3%"
    },
    {
        "thought": "By letting different LLMs debate with each other, we can leverage their diverse perspectives to find better codes for tasks.",
        "name": "LLM Debate",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    debate_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for debating and updating the code based on other agents' code\n    debate_instruction = \"Given code to the problem from other agents, consider their opinions as additional advice. Please think carefully and provide an updated answer.\"\n    \n    # Initialize debate agents with different roles and a moderate temperature for varied reasoning\n    debate_agents = [LLMAgentBase(['thinking', 'code'], 'Debate Agent', temperature=0.8, role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast']]\n\n    # Instruction for final decision-making based on all debates and code\n    final_decision_instruction = \"Given all the above thinking and code, reason over them carefully and provide a final code.\"\n    final_decision_agent = LLMAgentBase(['thinking', 'code'], 'Final Decision Agent', temperature=0.1)\n\n    max_round = 2 # Maximum number of debate rounds\n    all_thinking = [[] for _ in range(max_round)]\n    all_code = [[] for _ in range(max_round)]\n\n    # Perform debate rounds\n    for r in range(max_round):\n        for i in range(len(debate_agents)):\n            if r == 0:\n                thinking, code = debate_agents[i]([taskInfo], debate_initial_instruction)\n            else:\n                input_infos = [taskInfo] + [all_thinking[r-1][i]] + all_thinking[r-1][:i] + all_thinking[r-1][i+1:]\n                thinking, code = debate_agents[i](input_infos, debate_instruction)\n            all_thinking[r].append(thinking)\n            all_code[r].append(code)\n    \n    # Make the final decision based on all debate results and code\n    thinking, code = final_decision_agent([taskInfo] + all_thinking[max_round-1] + all_code[max_round-1], final_decision_instruction)\n    return code\n",
        "generation": "initial",
        "test_fitness": "90.8%"
    },
    {
        "thought": "Let LLM first think about the principles involved in solving this task which could be helpful. By understanding the underlying principles, the model can better reason through the problem and provide a more accurate answer.",
        "name": "Step-back Abstraction",
        "code": "def forward(self, taskInfo):\n        # Instruction for understanding the principles involved in the task\n        principle_instruction = \"What are the physics, chemistry or biology principles and concepts involved in solving this task? First think step by step. Then list all involved principles and explain them.\"\n        \n        # Instruction for solving the task based on the principles\n        cot_instruction = \"Given the question and the involved principle behind the question, think step by step and then solve the task.\"\n        \n        # Instantiate LLM agents\n        principle_agent = LLMAgentBase(['thinking', 'principle'], 'Principle Agent')\n        cot_agent = LLMAgentBase(['thinking', 'code'], 'Chain-of-Thought Agent')\n        \n        # Get the principles involved in the task\n        thinking, principle = principle_agent([taskInfo], principle_instruction)\n\n        # Use the principles to solve the task\n        thinking, code = cot_agent([taskInfo, thinking, principle], cot_instruction)\n        return code\n",
        "generation": "initial",
        "test_fitness": "87.8%"
    },
    {
        "thought": "Similar to Quality-Diversity methods, let LLM generate multiple diverse interesting code could help. By encouraging the model to explore different reasoning paths, we can increase the chances of finding the best answer.",
        "name": "Quality-Diversity",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for giving diverse code\n    qd_instruction = \"Given previous attempts, try to come up with another interesting way to solve the task.\"\n    cot_agent = LLMAgentBase(['thinking', 'code'], 'Chain-of-Thought Agent')\n\n    # Instruction for final decision-making based on collected reasoning and code\n    final_decision_instruction = \"Given all the above code, reason over them carefully and provide a final code.\"\n    final_decision_agent = LLMAgentBase(['thinking', 'code'], 'Final Decision Agent', temperature=0.1)\n    \n    N_max = 3 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    possible_code = []\n    thinking, code = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    # Add the code to the list of possible code\n    possible_code.extend([thinking, code])\n\n    for i in range(N_max):\n        # Reflect on previous attempts and generate another interesting code\n        cot_inputs.extend([thinking, code])\n\n        # Generate another interesting code\n        thinking, code = cot_agent(cot_inputs, qd_instruction, i + 1)\n        possible_code.extend([thinking, code])\n\n    # Make the final decision based on all generated code\n    thinking, code = final_decision_agent([taskInfo] + possible_code, final_decision_instruction)\n    return code\n",
        "generation": "initial",
        "test_fitness": "88.5%"
    },
    {
        "thought": "Similar to Auto-GPT and expert prompting, we can use dynamic control flow in the design to let the agent decide what expert we should use.",
        "name": "Dynamic Assignment of Roles",
        "code": "def forward(self, taskInfo):\n        # Instruction for step-by-step reasoning\n        cot_instruction = \"Please think step by step and then solve the task.\"\n        expert_agents = [LLMAgentBase(['thinking', 'code'], 'Expert Agent', role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast', 'Helpful Assistant']]\n\n        # Instruction for routing the task to the appropriate expert\n        routing_instruction = \"Given the task, please choose an Expert to code the question. Choose from: Math Professor, Grade School Teacher, Math Enthusiast.\"\n        routing_agent = LLMAgentBase(['choice'], 'Routing agent')\n\n        # Get the choice of expert to route the task\n        choice = routing_agent([taskInfo], routing_instruction)[0]\n\n        if 'professor' in choice.content.lower():\n            expert_id = 0\n        elif 'teacher' in choice.content.lower():\n            expert_id = 1\n        elif 'enthusiast' in choice.content.lower():\n            expert_id = 2\n        else:\n            expert_id = 3 # Default to helpful assistant\n\n        thinking, code = expert_agents[expert_id]([taskInfo], cot_instruction)\n        return code\n",
        "generation": "initial",
        "test_fitness": "89.3%"
    },
    {
        "thought": "**Insights:**\nThe top-performing agents across the HumanEval/MATH subtasks share several core features: (1) initial step-by-step reasoning (Chain-of-Thought), (2) iterative self-refinement using feedback (from either a critic or test-driven evaluation), and (3) modularity that allows for plugging in additional modules (such as principle abstraction for science tasks or diverse test case generation). However, the best solutions also show that not all modules are always needed: for most coding tasks, test-driven feedback is the most effective, while principle abstraction is only sometimes useful. Moreover, invoking both a critic and a test evaluator in every loop can be redundant and slow. \n\n**Overall Idea:**\nDesign a streamlined, adaptive agent that always starts with Chain-of-Thought reasoning and code generation, then uses test-driven feedback as the primary refinement mechanism. If the test-driven feedback is ambiguous or the code repeatedly fails, the agent can escalate to a critic for deeper analysis. Principle abstraction is invoked only if science-related keywords are detected. This design keeps the architecture simple and fast for most tasks, but robustly adaptable for harder or science-related problems. The agent maintains modularity by conditionally invoking extra modules only when needed, and avoids redundant LLM calls.\n\n**Implementation:**\n1. Generate initial code with step-by-step reasoning (CoT).\n2. If the task appears science-related, extract principles and add to the context.\n3. Generate diverse test cases.\n4. Simulate running the code on the test cases. If all pass, return the code.\n5. If any test fails, use the feedback to refine the code. If repeated failures occur (e.g., after 2 iterations), escalate to a critic for deeper analysis and incorporate that feedback.\n6. Repeat the test-refine loop for a fixed number of iterations or until all tests pass.\n7. Return the final code.\nThis approach keeps the architecture efficient for most tasks, but robustly adaptive for harder cases.",
        "name": "Adaptive Test-Driven Self-Refine with Conditional Critic and Principle Modules",
        "code": "def forward(self, taskInfo):\n    # 1. Initial code generation with CoT\n    cot_instruction = \"Please think step by step and then solve the task.\"\n    cot_agent = LLMAgentBase(['thinking', 'code'], 'Chain-of-Thought Agent')\n    cot_inputs = [taskInfo]\n\n    # 2. Optionally extract principles if task is science-related\n    task_text = taskInfo.content.lower()\n    science_keywords = ['physics', 'chemistry', 'biology', 'science', 'experiment', 'reaction', 'force', 'energy', 'molecule', 'atom']\n    use_principle = any(kw in task_text for kw in science_keywords)\n    if use_principle:\n        principle_instruction = (\n            \"What are the physics, chemistry or biology principles and concepts involved in solving this task? \"\n            \"First think step by step. Then list all involved principles and explain them.\"\n        )\n        principle_agent = LLMAgentBase(['thinking', 'principle'], 'Principle Agent')\n        principle_thinking, principle = principle_agent([taskInfo], principle_instruction)\n        cot_inputs.extend([principle_thinking, principle])\n\n    thinking, code = cot_agent(cot_inputs, cot_instruction, 0)\n\n    # 3. Generate test cases\n    testgen_instruction = (\n        \"Given the task description and any examples, generate a diverse set of 3-5 input/output test cases \"\n        \"(including edge cases) that would thoroughly test a correct solution. \"\n        \"Output each test case as a Python tuple: (input_arguments, expected_output).\"\n    )\n    testgen_agent = LLMAgentBase(['test_cases'], 'Test Generator Agent')\n    test_cases_info = testgen_agent([taskInfo], testgen_instruction, 0)[0]\n\n    # 4. Simulate running the code on the test cases\n    eval_instruction = (\n        \"Given the code, the task, and the test cases, simulate running the code on each test case. \"\n        \"For each, predict the output and compare to the expected output. \"\n        \"If any test fails, explain why and where the code might be wrong. \"\n        \"Return a JSON with a 'pass' boolean (True if all tests pass), and a 'feedback' string.\"\n    )\n    eval_agent = LLMAgentBase(['pass', 'feedback'], 'Test Evaluator Agent')\n\n    N_max = 4\n    critic_threshold = 2  # Escalate to critic after this many failures\n    for i in range(N_max):\n        eval_infos = eval_agent([taskInfo, code, test_cases_info], eval_instruction, i)\n        pass_info, feedback_info = eval_infos\n        if pass_info.content == True or pass_info.content == 'True':\n            break\n        # 5. If repeated failures, escalate to critic\n        if i >= critic_threshold:\n            critic_instruction = (\n                \"Please review the code above and the feedback from the test evaluator. \"\n                \"Criticize where it might be wrong or could be improved. If you are absolutely sure it is correct, output 'True' in 'correct'.\"\n            )\n            critic_agent = LLMAgentBase(['feedback', 'correct'], 'Critic Agent')\n            critic_feedback, correct = critic_agent([taskInfo, code, feedback_info], critic_instruction, i)\n            refine_inputs = [taskInfo, code, feedback_info, critic_feedback]\n            if use_principle:\n                refine_inputs.append(principle)\n        else:\n            refine_inputs = [taskInfo, code, feedback_info]\n            if use_principle:\n                refine_inputs.append(principle)\n        refine_instruction = (\n            \"Given the task, your previous code, and the feedback from failed tests\" +\n            (\", and any additional critique\" if i >= critic_threshold else \"\") +\n            \", reflect on what went wrong and write improved code. Think step by step, then output your revised code.\"\n        )\n        refiner_agent = LLMAgentBase(['thinking', 'code'], 'Refiner Agent')\n        thinking, code = refiner_agent(refine_inputs, refine_instruction, i+1)\n    return code\n",
        "generation": 3,
        "label": "outer_loop",
        "special_label": "fail_reflexion",
        "test_fitness": "94.7%"
    }
]